<p style="font-family: Avenir, sans-serif; font-size: 14px;">
  For a full publication list, please refer to my 
  <a href="https://scholar.google.com/citations?user=A9D-disAAAAJ&hl=zh-CN" 
     style="color: inherit; text-decoration: underline;">Google Scholar</a>.
</p>

<b style="background: linear-gradient(120deg, #4b6cb7, #8a4fff); 
                 -webkit-background-clip: text; 
                 background-clip: text; 
                 color: transparent;
                 font-weight: 600;">Ming-Flash-Omni</b><b>: A Sparse, Unified Architecture for Multimodal Perception and Generation</b> <br> 
[[paper](https://arxiv.org/pdf/2510.24821)][[code](https://github.com/inclusionAI/Ming)][[hf](https://huggingface.co/inclusionAI/Ming-flash-omni-Preview)]
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> A 100-billion-scale omni-modal MoE model delivering leading results in text-to-image generation, generative segmentation and contextual ASR. </i>
<span style="font-family:avenir; font-size: 13px; line-height: 1.3;">Ming team, Ant Group.</span><br>

<b style="background: linear-gradient(120deg, #4b6cb7, #8a4fff); 
                 -webkit-background-clip: text; 
                 background-clip: text; 
                 color: transparent;
                 font-weight: 600;">Ming-UniAudio</b><b>: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation</b><br>
[[paper](https://arxiv.org/pdf/2511.05516)][[code](https://github.com/inclusionAI/Ming-UniAudio)][[hf](https://huggingface.co/inclusionAI/Ming-UniAudio-16B-A3B)]
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> A unified audio model for understanding, generating, and editing audio contents, based on unified representations. </i>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Canxiang Yan, Chunxiang Jin, Dawei Huang, Haibing Yu, Han Peng, Hui Zhan, Jie Gao, Jing Peng, Jingdong Chen, Jun Zhou, Kaimeng Ren, Ming Yang, Mingxue Yang, Qiang Xu, Qin Zhao, Ruijie Xiong, Shaoxiong Lin, Xuezhi Wang, Yi Yuan, Yifei Wu, Yongjie Lyu, Zhengyu He, Zhihao Qiu, Zhiqiang Fang, <b>Ziyuan Huang</b></span><br>

<b style="background: linear-gradient(120deg, #4b6cb7, #8a4fff); 
                 -webkit-background-clip: text; 
                 background-clip: text; 
                 color: transparent;
                 font-weight: 600;">Ming-UniVision</b><b>: Joint Image Understanding and Generation with a Unified Continuous Tokenizer</b><br>
[[paper](https://arxiv.org/pdf/2510.06590)][[code](https://github.com/inclusionAI/Ming-UniVision)][[hf](https://huggingface.co/inclusionAI/Ming-UniVision-16B-A3B)]
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> A unified MLLM for understanding, generating and editing visual contents that seamlessly supports multi-round interactions, all powered by the first-ever continuous unified visual representations. </i>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;"><b>Ziyuan Huang</b>, DanDan Zheng, Cheng Zou, Rui Liu, Xiaolong Wang, Kaixiang Ji, Weilong Chai, Jianxin Sun, Libin Wang, Yongjie Lyv, Taoye Huang, Jiajia Liu, Qingpei Guo, Ming Yang, Jingdong Chen, Jun Zhou</span><br>

<b style="background: linear-gradient(120deg, #4b6cb7, #8a4fff); 
                 -webkit-background-clip: text; 
                 background-clip: text; 
                 color: transparent;
                 font-weight: 600;">Ming-Omni</b><b>: A Unified Multimodal Model for Perception and Generation</b> <br> 
[[paper](https://arxiv.org/abs/2506.09344)][[code](https://github.com/inclusionAI/Ming)][[hf](https://huggingface.co/inclusionAI/Ming-Lite-Omni)]
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> The first open-source omni-modal model that matches the input-output capability of GPT-4o. </i>
<span style="font-family:avenir; font-size: 13px; line-height: 1.3;">Ming team, Ant Group.</span><br>

<b>ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</b><br>
[[paper](https://arxiv.org/pdf/2510.20803)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> NeurIPS 2025. </i>

<b>Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight</b><br>
[[paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/8a54a80ffc2834689ffdd0920202018e-Paper-Conference.pdf)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Ziyuan Huang, Kaixiang Ji, Biao Gong, Zhiwu Qing, Qinglong Zhang, Kecheng Zheng, Jian Wang, Jingdong Chen, Ming Yang</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> NeurIPS 2024. </i>

<b>Skysense: A multi-modal remote sensing foundation model towards universal interpretation for earth observation imagery</b><br>
[[paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_SkySense_A_Multi-Modal_Remote_Sensing_Foundation_Model_Towards_Universal_Interpretation_CVPR_2024_paper.pdf)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, Huimei He, Jian Wang, Jingdong Chen, Ming Yang, Yongjun Zhang, Yansheng Li
</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> CVPR 2024. </i>

<b>Res-tuning: A flexible and efficient tuning paradigm via unbinding tuner from backbone</b><br>
[[paper](https://arxiv.org/abs/2310.19859)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang Lv, Yujun Shen, Deli Zhao, Jingren Zhou
</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> NeurIPS 2024. </i>

<b>Towards real-world visual tracking with temporal contexts</b><br>
[[paper](https://arxiv.org/pdf/2308.10330)][[code](https://github.com/vision4robotics/TCTrack)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Ziang Cao, Ziyuan Huang, Liang Pan, Shiwei Zhang, Ziwei Liu, Changhong Fu
</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> TPAMI. </i>

<b>MAR: Masked Autoencoders for Efficient Action Recognition</b><br>
[[paper](https://arxiv.org/pdf/2207.11660)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Xiang Wang, Yuehuan Wang, Yiliang Lv, Changxin Gao, Nong Sang
</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> Transactions on Multimedia. </i>

<b>Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning</b><br>
[[paper](http://openaccess.thecvf.com/content/ICCV2023/papers/Qing_Disentangling_Spatial_and_Temporal_Learning_for_Efficient_Image-to-Video_Transfer_Learning_ICCV_2023_paper.pdf)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Yingya Zhang, Changxin Gao, Deli Zhao, Nong Sang
</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> ICCV 2023. </i>

<b>PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework</b><br>
[[paper](http://openaccess.thecvf.com/content/ICCV2023/papers/Li_PVT_A_Simple_End-to-End_Latency-Aware_Visual_Tracking_Framework_ICCV_2023_paper.pdf)][[code](https://github.com/Jaraxxus-Me/PVT_pp)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Bowen Li*, Ziyuan Huang*, Junjie Ye, Yiming Li, Sebastian Scherer, Hang Zhao, Changhong Fu
</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> ICCV 2023. </i>


<b>TAda! Temporally-Adaptive Convolutions for Video Understanding</b><br>
[[paper](https://arxiv.org/pdf/2110.06178)][[code](https://github.com/alibaba-mmai-research/TAdaConv)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Ziyuan Huang, Shiwei Zhang, Liang Pan, Zhiwu Qing, Mingqian Tang, Ziwei Liu, Marcelo H Ang Jr
</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> ICLR 2023. </i>

<b>Support-Set Based Cross-Supervision for Video Grounding</b><br>
[[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_Support-Set_Based_Cross-Supervision_for_Video_Grounding_ICCV_2021_paper.pdf)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Xinpeng Ding, Nannan Wang, Shiwei Zhang, De Cheng, Xiaomeng Li, Ziyuan Huang, Mingqian Tang, Xinbo Gao
</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> ICCV 2021. </i>

<b>Self-supervised Motion Learning from Static Images</b><br>
[[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Self-Supervised_Motion_Learning_From_Static_Images_CVPR_2021_paper.pdf)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Ziyuan Huang, Shiwei Zhang, Jianwen Jiang, Mingqian Tang, Rong Jin, Marcelo Ang
</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> CVPR 2021. </i>

<b>Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation for Autonomous Driving Applications</b><br>
[[paper](https://arxiv.org/pdf/2004.05560)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Feng Xue, Guirong Zhuo, Ziyuan Huang, Wufei Fu, Zhuoyue Wu, Marcelo H Ang Jr
</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> IROS 2020. </i>

<b>Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking</b><br>
[[paper](http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Learning_Aberrance_Repressed_Correlation_Filters_for_Real-Time_UAV_Tracking_ICCV_2019_paper.pdf)]<br>
<span style="font-size: 13px; line-height: 1.3; margin-top: 4px; margin-bottom: 4px;">Ziyuan Huang, Changhong Fu, Yiming Li, Fuling Lin, Peng Lu
</span><br>
<i style="font-family:avenir; font-size: 14px; color: #666; line-height: 1.4; margin-top: 4px; margin-bottom: 4px; display: block;"> ICCV 2019. </i>