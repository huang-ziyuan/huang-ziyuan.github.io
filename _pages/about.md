---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!-- <p style="font-family:avenir;">
Hi there! I am Ziyuan Huang (黄子渊), currently a research scientist at Ant Group, building unified multi-modal models towards AGI. 
I received my Ph.D. from National University of Singapore in 2023, where I was advised by Prof. Marcelo Ang. 
My main research interests are on 
</p>

<p style="font-family:avenir;">
Prior to Ant, I have spent wonderful times conducting research in the MARS Lab with Professor Zhao Hang, TONGYI with Dr. Zhang Shiwei, and Vision4Robotics Group at Tongji University with Professor Fu Changhong. I am also fortunate to have worked closely with Dr. Pan Liang and Professor Liu Ziwei in S-Lab@NTU.
</p>

<script src="https://kit.fontawesome.com/44809c8ce4.js" crossorigin="anonymous"></script>

<p style="font-family:avenir;color:green;"> <i class="fa-solid fa-bullhorn"></i> 
We are actively hiring self-motivated full-time engineers and interns to work on cutting-edge research projects on unified multi-modal models. <br>
Feel free to drop me an email if you are interested! </p> -->

<p style="font-family:avenir; font-size: 16px;">
I am Ziyuan Huang, a research scientist at Ant Group, <strong>
    <span style="background: linear-gradient(120deg, #4b6cb7, #8a4fff); 
                 -webkit-background-clip: text; 
                 background-clip: text; 
                 color: transparent;
                 font-weight: 600;">
      advancing omni-modal intelligence
    </span>
  </strong> - next frontier of artificial general intelligence.
  My work focuses on a foundational breakthrough: a unified model grounded in <strong>unified representations</strong> that unlocks deep cross-modal and cross-task synergy, moving beyond isolated perception or generation pipelines. 
  This enables AI systems that can truly <strong>assist, create and collaborate with people</strong> in complex, real-world contexts, intuitively, coherently, and across any modality.
  <!-- a unified AI framework that supports joint understanding and generation across vision, language, audio, and other modalities.  -->
<!-- My goal is to develop models grounded in <strong>unified representations</strong>, where cross-modal and cross-task synergy emerges naturally from shared latent structures, enabling more coherent, generalizable, and efficient multi-modal reasoning.  -->
<!-- Ultimately, I believe such systems are a crucial step toward scalable and robust AGI system. -->
</p>

<p style="font-family:avenir; font-size: 16px;">

I earned my Ph.D. degree from National University of Singapore in 2023 under the supervision of Prof. Marcelo Ang. Prior to Ant, I have spent wonderful times conducting research in the MARS Lab under Professor Zhao Hang, TONGYI under Dr. Zhang Shiwei, and Vision4Robotics Group at Tongji University under Professor Fu Changhong. I am also fortunate to have worked closely with Dr. Pan Liang and Professor Liu Ziwei in S-Lab@NTU.
</p>

<script src="https://kit.fontawesome.com/44809c8ce4.js" crossorigin="anonymous"></script>

<p style="font-family:avenir;color:green; font-size: 16px;"> <i class="fa-solid fa-bullhorn"></i> We are actively hiring self-motivated full-time research scientists and interns to work on cutting-edge research projects on unified omni-modal models. Feel free to drop me an email if you are interested! </p>

## Selected technical reports

<p style="font-family: Avenir, sans-serif; font-size: 14px;">
  For a full publication list, please refer to my 
  <a href="https://scholar.google.com/citations?user=A9D-disAAAAJ&hl=zh-CN" 
     style="color: inherit; text-decoration: underline;">Google Scholar</a>.
</p>

<!-- Publication: Ming-Flash-Omni -->
<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/ming-flash-omni-thumbnail.png" 
         alt="Ming-Flash-Omni" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      <b style="background: linear-gradient(120deg, #4b6cb7, #8a4fff); -webkit-background-clip: text; background-clip: text; color: transparent;">Ming-Flash-Omni</b>: A Sparse, Unified Architecture for Multimodal Perception and Generation
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Ming team, Ant Group.
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      A 100-billion-scale omni-modal MoE model delivering leading results in text-to-image generation, generative segmentation and contextual ASR.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://arxiv.org/pdf/2510.24821" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/inclusionAI/Ming" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
      <a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[hf]</a>
    </p>
  </div>
</div>

<!-- Publication: Ming-UniAudio -->
<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/ming-uniaudio-thumbnail.png" 
         alt="Ming-UniAudio" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      <b style="background: linear-gradient(120deg, #4b6cb7, #8a4fff); -webkit-background-clip: text; background-clip: text; color: transparent;">Ming-UniAudio</b>: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Canxiang Yan, Chunxiang Jin, Dawei Huang, Haibing Yu, Han Peng, Hui Zhan, Jie Gao, Jing Peng, Jingdong Chen, Jun Zhou, Kaimeng Ren, Ming Yang, Mingxue Yang, Qiang Xu, Qin Zhao, Ruijie Xiong, Shaoxiong Lin, Xuezhi Wang, Yi Yuan, Yifei Wu, Yongjie Lyu, Zhengyu He, Zhihao Qiu, Zhiqiang Fang, <b>Ziyuan Huang</b>
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      A unified audio model for understanding, generating, and editing audio contents, based on unified representations.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://arxiv.org/pdf/2511.05516" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/inclusionAI/Ming-UniAudio" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
      <a href="https://huggingface.co/inclusionAI/Ming-UniAudio-16B-A3B" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[hf]</a>
    </p>
  </div>
</div>

<!-- Publication: Ming-UniVision -->
<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/ming-univision-thumbnail.png" 
         alt="Ming-UniVision" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      <b style="background: linear-gradient(120deg, #4b6cb7, #8a4fff); -webkit-background-clip: text; background-clip: text; color: transparent;">Ming-UniVision</b>: Joint Image Understanding and Generation with a Unified Continuous Tokenizer
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      <b>Ziyuan Huang</b>, DanDan Zheng, Cheng Zou, Rui Liu, Xiaolong Wang, Kaixiang Ji, Weilong Chai, Jianxin Sun, Libin Wang, Yongjie Lyv, Taoye Huang, Jiajia Liu, Qingpei Guo, Ming Yang, Jingdong Chen, Jun Zhou
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      A unified MLLM for understanding, generating and editing visual contents that seamlessly supports multi-round interactions, all powered by the first-ever continuous unified visual representations.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://arxiv.org/pdf/2510.06590" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/inclusionAI/Ming-UniVision" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
      <a href="https://huggingface.co/inclusionAI/Ming-UniVision-16B-A3B" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[hf]</a>
    </p>
  </div>
</div>

<!-- Publication: Ming-Omni -->
<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/ming-omni-thumbnail.png" 
         alt="Ming-Omni" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      <b style="background: linear-gradient(120deg, #4b6cb7, #8a4fff); -webkit-background-clip: text; background-clip: text; color: transparent;">Ming-Omni</b>: A Unified Multimodal Model for Perception and Generation
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Ming team, Ant Group.
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      The first open-source omni-modal model that matches the input-output capability of GPT-4o.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://arxiv.org/abs/2506.09344" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/inclusionAI/Ming" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
      <a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[hf]</a>
    </p>
  </div>
</div>

## Selected publications

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/argenseg-2-thumbnail.png" 
         alt="ARGenSeg" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      ARGenSeg: Image Segmentation with Autoregressive Image Generation Model
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      NeurIPS 2025.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://arxiv.org/pdf/2510.20803" style="color: #2e8b57; text-decoration: none;">[paper]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/skipvision-thumbnail.png" 
         alt="SkipVision" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      ICCV 2025.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://arxiv.org/pdf/2503.21817" style="color: #2e8b57; text-decoration: none;">[paper]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/cos-thumbnail.png" 
         alt="Chain-of-Sight" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Ziyuan Huang, Kaixiang Ji, Biao Gong, Zhiwu Qing, Qinglong Zhang, Kecheng Zheng, Jian Wang, Jingdong Chen, Ming Yang
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      NeurIPS 2024.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/8a54a80ffc2834689ffdd0920202018e-Paper-Conference.pdf" style="color: #2e8b57; text-decoration: none;">[paper]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/skysense-thumbnail.png" 
         alt="SkySense" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      Skysense: A multi-modal remote sensing foundation model towards universal interpretation for earth observation imagery
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, Huimei He, Jian Wang, Jingdong Chen, Ming Yang, Yongjun Zhang, Yansheng Li
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      CVPR 2024.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_SkySense_A_Multi-Modal_Remote_Sensing_Foundation_Model_Towards_Universal_Interpretation_CVPR_2024_paper.pdf" style="color: #2e8b57; text-decoration: none;">[paper]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/restuning-thumbnail.png" 
         alt="Res-Tuning" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      Res-tuning: A flexible and efficient tuning paradigm via unbinding tuner from backbone
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang Lv, Yujun Shen, Deli Zhao, Jingren Zhou
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      NeurIPS 2024.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://arxiv.org/abs/2310.19859" style="color: #2e8b57; text-decoration: none;">[paper]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/tctrack-thumbnail.png" 
         alt="TCTrack" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      Towards real-world visual tracking with temporal contexts
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Ziang Cao, Ziyuan Huang, Liang Pan, Shiwei Zhang, Ziwei Liu, Changhong Fu
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      TPAMI.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://arxiv.org/pdf/2308.10330" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/vision4robotics/TCTrack" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/mar-thumbnail.png" 
         alt="MAR" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      MAR: Masked Autoencoders for Efficient Action Recognition
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Xiang Wang, Yuehuan Wang, Yiliang Lv, Changxin Gao, Nong Sang
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      Transactions on Multimedia.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://arxiv.org/pdf/2207.11660" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/alibaba-mmai-research/Masked-Action-Recognition" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/dist-thumbnail.png" 
         alt="DiST" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Yingya Zhang, Changxin Gao, Deli Zhao, Nong Sang
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      ICCV 2023.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="http://openaccess.thecvf.com/content/ICCV2023/papers/Qing_Disentangling_Spatial_and_Temporal_Learning_for_Efficient_Image-to-Video_Transfer_Learning_ICCV_2023_paper.pdf" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/alibaba-mmai-research/DiST" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/pvtpp-thumbnail.png" 
         alt="PVTpp" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Bowen Li*, Ziyuan Huang*, Junjie Ye, Yiming Li, Sebastian Scherer, Hang Zhao, Changhong Fu
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      ICCV 2023.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="http://openaccess.thecvf.com/content/ICCV2023/papers/Li_PVT_A_Simple_End-to-End_Latency-Aware_Visual_Tracking_Framework_ICCV_2023_paper.pdf" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/Jaraxxus-Me/PVT_pp" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/tadaconv-thumbnail.png" 
         alt="TAdaConv" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      TAda! Temporally-Adaptive Convolutions for Video Understanding
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Ziyuan Huang, Shiwei Zhang, Liang Pan, Zhiwu Qing, Mingqian Tang, Ziwei Liu, Marcelo H Ang Jr
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      ICLR 2022.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://arxiv.org/pdf/2110.06178" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/alibaba-mmai-research/TAdaConv" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/sscs-thumbnail.png" 
         alt="SSCS" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      Support-Set Based Cross-Supervision for Video Grounding
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Xinpeng Ding, Nannan Wang, Shiwei Zhang, De Cheng, Xiaomeng Li, Ziyuan Huang, Mingqian Tang, Xinbo Gao
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      ICCV 2021.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_Support-Set_Based_Cross-Supervision_for_Video_Grounding_ICCV_2021_paper.pdf" style="color: #2e8b57; text-decoration: none;">[paper]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/mosi-thumbnail.png" 
         alt="MoSI" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      Self-supervised Motion Learning from Static Images
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Ziyuan Huang, Shiwei Zhang, Liang Pan, Zhiwu Qing, Mingqian Tang, Ziwei Liu, Marcelo H Ang Jr
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      CVPR 2021.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Self-Supervised_Motion_Learning_From_Static_Images_CVPR_2021_paper.pdf" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/alibaba-mmai-research/TAdaConv" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/dnet-thumbnail.png" 
         alt="DNet" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation for Autonomous Driving Applications
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Feng Xue, Guirong Zhuo, Ziyuan Huang, Wufei Fu, Zhuoyue Wu, Marcelo H Ang Jr
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      IROS 2020.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="https://arxiv.org/pdf/2004.05560" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/TJ-IPLab/DNet" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
    </p>
  </div>
</div>

<div style="display: flex; gap: 16px; margin-bottom: 32px; align-items: flex-start;">
  <div style="width: 150px; flex-shrink: 0;">
    <img src="images/arcf-thumbnail.png" 
         alt="ARCF" 
         style="width: 100%; height: auto; border-radius: 8px; display: block;">
  </div>
  <div style="flex: 1; min-width: 0; font-family: Avenir, sans-serif;">
    <p style="margin: 0; font-size: 16px; font-weight: 600;">
      Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking
    </p>
    <p style="margin: 4px 0; font-size: 13px; color: #666; line-height: 1.3;">
      Ziyuan Huang, Changhong Fu, Yiming Li, Fuling Lin, Peng Lu
    </p>
    <i style="font-size: 14px; color: #666; line-height: 1.4; display: block; margin: 6px 0;">
      ICCV 2019.
    </i>
    <p style="margin: 6px 0; font-size: 14px;">
      <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Learning_Aberrance_Repressed_Correlation_Filters_for_Real-Time_UAV_Tracking_ICCV_2019_paper.pdf" style="color: #2e8b57; text-decoration: none;">[paper]</a>
      <a href="https://github.com/vision4robotics/ARCF-tracker" style="color: #2e8b57; text-decoration: none; margin-left: 8px;">[code]</a>
    </p>
  </div>
</div>
